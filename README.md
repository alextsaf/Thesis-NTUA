# Abstract

In recent years, cloud computing has become one of the most influential fields in computer science. Cloud services are increasingly popular, and the workload of providers continues to grow steadily. According to the International Energy Agency, data center energy consumption accounts for 1-1.5% of global energy consumption, which is expected to, in some cases, rise by six times by 2031. The need for optimal resource management is more pressing than ever, requiring accurate workload forecasting. In this dissertation, we focused on predicting multivariate workload time series data and explored the feasibility of implementing an automated resource management system. Four different linear regression models (Linear Regression, Lasso Regression, Ridge Regression, ElasticNet Regression), as well as neural networks (DNN, CNN, LSTM) were examined, whose performance was compared for each dataset using three metrics (MAE, ME, PRMSE). Their fast fit time allowed the selection of the optimal prediction model for each computing system after comparison. Using the predictions, we showed that an automated management system could decrease the used resources by up to 60% compared to systems that do not use a smart management policy. The data were generated by analyzing and processing real-world distributed systems log data as well as the creation of synthetic time series data. Finally, we examined and mentioned future extensions of the project for the implementation automatic resource provisioning system.


## Keywords
Cloud Computing, Data Analysis, Time Series Forecasting, Machine Learning, Linear Regression, Deep Neural Networks, Recurrent Neural Networks, Convolutional Neural Networks, LSTM
