{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeDataset(path, timeColumn = 'SubmitTime', delimiter = ','):\n",
    "\n",
    "    datasetname = path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    #Lambda function to convert seconds from 1970 to timestamp\n",
    "    convert_to_timestamp = lambda x: pd.to_datetime(x, unit='s')\n",
    "\n",
    "    # Read the dataset into a dataframe and set the timestamp as the index.\n",
    "    dataset = pd.read_csv(path, parse_dates=[timeColumn], date_parser=convert_to_timestamp, delimiter=delimiter)\n",
    "\n",
    "    # Print the number of rows and columns in the dataframe.\n",
    "    print(\"Dataset Shape:\")\n",
    "    print(dataset.shape)\n",
    "\n",
    "    # Print the number of missing values in dataframe.\n",
    "    print(\"Number of missing values:\")\n",
    "    print(dataset.isnull().sum().sum())\n",
    "\n",
    "    #Print the names of the columns in the dataframe.\n",
    "    print(\"Column Names:\")\n",
    "    print(dataset.columns)\n",
    "\n",
    "    # tex row\n",
    "    jobsInMillions = round(dataset.shape[0] / 1000000, 2)\n",
    "    submitTimesInYears = round((dataset[timeColumn].max() - dataset[timeColumn].min()).days / 365, 2)\n",
    "    print(\"Latex Row:\")\n",
    "    print(f\" \\hline \\n {datasetname} & {jobsInMillions} M & {submitTimesInYears} yrs & \\\\\\\\\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:\n",
      "(1124772, 29)\n",
      "Number of missing values:\n",
      "0\n",
      "Column Names:\n",
      "Index(['JobID', 'SubmitTime', 'WaitTime', 'RunTime', 'NProc', 'UsedCPUTime',\n",
      "       'UsedMemory', 'ReqNProcs', 'ReqTime', 'ReqMemory', 'Status', 'UserID',\n",
      "       'GroupID', 'ExecutableID', 'QueueID', 'PartitionID', 'OrigSiteID',\n",
      "       'LastRunSiteID', 'JobStructure', 'JobStructureParams', 'UsedNetwork',\n",
      "       'UsedLocalDiskSpace', 'UsedResources', 'ReqPlatform', 'ReqNetwork',\n",
      "       'ReqLocalDiskSpace', 'ReqResources', 'VOID', 'ProjectID'],\n",
      "      dtype='object')\n",
      "Latex Row:\n",
      " \\hline \n",
      " das2 & 1.12 M & 1.8 yrs & \\\\\n"
     ]
    }
   ],
   "source": [
    "analyzeDataset('./datasets/das2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:\n",
      "(1020195, 29)\n",
      "Number of missing values:\n",
      "0\n",
      "Column Names:\n",
      "Index(['JobID', 'SubmitTime', 'WaitTime', 'RunTime', 'NProc', 'UsedCPUTime',\n",
      "       'UsedMemory', 'ReqNProcs', 'ReqTime', 'ReqMemory', 'Status', 'UserID',\n",
      "       'GroupID', 'ExecutableID', 'QueueID', 'PartitionID', 'OrigSiteID',\n",
      "       'LastRunSiteID', 'JobStructure', 'JobStructureParams', 'UsedNetwork',\n",
      "       'UsedLocalDiskSpace', 'UsedResources', 'ReqPlatform', 'ReqNetwork',\n",
      "       'ReqLocalDiskSpace', 'ReqResources', 'VOID', 'ProjectID'],\n",
      "      dtype='object')\n",
      "Latex Row:\n",
      " \\hline \n",
      " grid5000 & 1.02 M & 2.52 yrs & \\\\\n"
     ]
    }
   ],
   "source": [
    "analyzeDataset('./datasets/grid5000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:\n",
      "(781370, 29)\n",
      "Number of missing values:\n",
      "0\n",
      "Column Names:\n",
      "Index(['JobID', 'SubmitTime', 'WaitTime', 'RunTime', 'NProc', 'UsedCPUTime',\n",
      "       'UsedMemory', 'ReqNProcs', 'ReqTime', 'ReqMemory', 'Status', 'UserID',\n",
      "       'GroupID', 'ExecutableID', 'QueueID', 'PartitionID', 'OrigSiteID',\n",
      "       'LastRunSiteID', 'JobStructure', 'JobStructureParams', 'UsedNetwork',\n",
      "       'UsedLocalDiskSpace', 'UsedResources', 'ReqPlatform', 'ReqNetwork',\n",
      "       'ReqLocalDiskSpace', 'ReqResources', 'VOID', 'ProjectID'],\n",
      "      dtype='object')\n",
      "Latex Row:\n",
      " \\hline \n",
      " nordugrid & 0.78 M & 3.15 yrs & \\\\\n"
     ]
    }
   ],
   "source": [
    "analyzeDataset('./datasets/nordugrid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:\n",
      "(404176, 29)\n",
      "Number of missing values:\n",
      "0\n",
      "Column Names:\n",
      "Index(['JobID', 'SubmitTime', 'WaitTime', 'RunTime', 'NProc', 'UsedCPUTime',\n",
      "       'UsedMemory', 'ReqNProcs', 'ReqTime', 'ReqMemory', 'Status', 'UserID',\n",
      "       'GroupID', 'ExecutableID', 'QueueID', 'PartitionID', 'OrigSiteID',\n",
      "       'LastRunSiteID', 'JobStructure', 'JobStructureParams', 'UsedNetwork',\n",
      "       'UsedLocalDiskSpace', 'UsedResources', 'ReqPlatform', 'ReqNetwork',\n",
      "       'ReqLocalDiskSpace', 'ReqResources', 'VOID', 'ProjectID'],\n",
      "      dtype='object')\n",
      "Latex Row:\n",
      " \\hline \n",
      " auvergrid & 0.4 M & 1.0 yrs & \\\\\n"
     ]
    }
   ],
   "source": [
    "analyzeDataset('./datasets/auvergrid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:\n",
      "(1195242, 29)\n",
      "Number of missing values:\n",
      "0\n",
      "Column Names:\n",
      "Index(['JobID', 'SubmitTime', 'WaitTime', 'RunTime', 'NProc', 'UsedCPUTime',\n",
      "       'UsedMemory', 'ReqNProcs', 'ReqTime', 'ReqMemory', 'Status', 'UserID',\n",
      "       'GroupID', 'ExecutableID', 'QueueID', 'PartitionID', 'OrigSiteID',\n",
      "       'LastRunSiteID', 'JobStructure', 'JobStructureParams', 'UsedNetwork',\n",
      "       'UsedLocalDiskSpace', 'UsedResources', 'ReqPlatform', 'ReqNetwork',\n",
      "       'ReqLocalDiskSpace', 'ReqResources', 'VOID', 'ProjectID'],\n",
      "      dtype='object')\n",
      "Latex Row:\n",
      " \\hline \n",
      " sharcnet & 1.2 M & 1.07 yrs & \\\\\n"
     ]
    }
   ],
   "source": [
    "analyzeDataset('./datasets/sharcnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:\n",
      "(188041, 29)\n",
      "Number of missing values:\n",
      "0\n",
      "Column Names:\n",
      "Index(['JobID', 'SubmitTime', 'WaitTime', 'RunTime', 'NProc', 'UsedCPUTime',\n",
      "       'UsedMemory', 'ReqNProcs', 'ReqTime', 'ReqMemory', 'Status', 'UserID',\n",
      "       'GroupID', 'ExecutableID', 'QueueID', 'PartitionID', 'OrigSiteID',\n",
      "       'LastRunSiteID', 'JobStructure', 'JobStructureParams', 'UsedNetwork',\n",
      "       'UsedLocalDiskSpace', 'UsedResources', 'ReqPlatform', 'ReqNetwork',\n",
      "       'ReqLocalDiskSpace', 'ReqResources', 'VOID', 'ProjectID'],\n",
      "      dtype='object')\n",
      "Latex Row:\n",
      " \\hline \n",
      " lcg & 0.19 M & 0.03 yrs & \\\\\n"
     ]
    }
   ],
   "source": [
    "analyzeDataset('./datasets/lcg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 500 csv's into one\n",
    "def mergeCSVs(path):\n",
    "    csvList = []\n",
    "    for csv in glob.glob(f\"./datasets/{path}/*/*.csv\"):\n",
    "        dataframe = pd.read_csv(csv, delimiter=';\\t')\n",
    "        csvList.append(dataframe)\n",
    "    df = pd.concat(csvList)\n",
    "    df.rename(columns={'Timestamp [ms]': 'SubmitTime'}, inplace=True)\n",
    "    df.to_csv( f\"./datasets/{path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# mergeCSVs('fastStorage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:\n",
      "(11221800, 11)\n",
      "Number of missing values:\n",
      "0\n",
      "Column Names:\n",
      "Index(['SubmitTime', 'CPU cores', 'CPU capacity provisioned [MHZ]',\n",
      "       'CPU usage [MHZ]', 'CPU usage [%]', 'Memory capacity provisioned [KB]',\n",
      "       'Memory usage [KB]', 'Disk read throughput [KB/s]',\n",
      "       'Disk write throughput [KB/s]', 'Network received throughput [KB/s]',\n",
      "       'Network transmitted throughput [KB/s]'],\n",
      "      dtype='object')\n",
      "Latex Row:\n",
      " \\hline \n",
      " fastStorage & 11.22 M & 0.08 yrs & \\\\\n"
     ]
    }
   ],
   "source": [
    "analyzeDataset('./datasets/fastStorage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# mergeCSVs('rnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:\n",
      "(12496728, 11)\n",
      "Number of missing values:\n",
      "0\n",
      "Column Names:\n",
      "Index(['SubmitTime', 'CPU cores', 'CPU capacity provisioned [MHZ]',\n",
      "       'CPU usage [MHZ]', 'CPU usage [%]', 'Memory capacity provisioned [KB]',\n",
      "       'Memory usage [KB]', 'Disk read throughput [KB/s]',\n",
      "       'Disk write throughput [KB/s]', 'Network received throughput [KB/s]',\n",
      "       'Network transmitted throughput [KB/s]'],\n",
      "      dtype='object')\n",
      "Latex Row:\n",
      " \\hline \n",
      " rnd & 12.5 M & 0.25 yrs & \\\\\n"
     ]
    }
   ],
   "source": [
    "analyzeDataset('./datasets/rnd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add the seconds of the row to the timestamp of the row \n",
    "\n",
    "def addSecondsToTimestamp(timestamp, secondsToAdd):\n",
    "    return timestamp + pd.Timedelta(seconds=secondsToAdd)\n",
    "\n",
    "\n",
    "def formDataset(path):\n",
    "    #Lambda function to convert seconds from 1970 to timestamp\n",
    "    convert_to_timestamp = lambda x: pd.to_datetime(x, unit='s')\n",
    "    dataset = pd.read_csv(path, delimiter=',', parse_dates=['SubmitTime'], date_parser=convert_to_timestamp)\n",
    "\n",
    "    dataset['StopTime'] = dataset[['SubmitTime', 'RunTime']].apply(lambda x: addSecondsToTimestamp(*x), axis=1)\n",
    "\n",
    "    dataset.to_csv( f\"./datasets/formed-{path.split('/')[-1]}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           SubmitTime  RunTime            StopTime\n",
      "0 2005-02-22 15:52:25       21 2005-02-22 15:52:46\n",
      "1 2005-02-22 15:52:26       21 2005-02-22 15:52:47\n",
      "2 2005-02-22 15:53:29        1 2005-02-22 15:53:30\n",
      "3 2005-02-22 16:53:49        0 2005-02-22 16:53:49\n",
      "4 2005-02-22 16:53:34        1 2005-02-22 16:53:35\n"
     ]
    }
   ],
   "source": [
    "formDataset('./datasets/das2.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
